Let $X_{it} = [X_{it1}, \cdots, X_{itD}]^\top$ be the measurements of player $i$ at the $t$-th time period, 
where $D$ is the number of features. Suppose we have the measurements of player $i$ from $1$ to the 
$T$-th time period\footnote{Note that for different users, the total time periods are not necessarily the same. 
Here for simplicity we assume that all users are measured during the same amount of time},
denoted as $X_{i} = [X_{i1}, \cdots, X_{iT}]^\top$. For a total number of $N$ players, we have their time series 
data $X = \{ X_1, \cdots, X_N \}$.

Our method includes two steps corresponding to the two tasks: 1) using HMM model to identify gamers' 
playing states at each time period, and 2) discovering player groups based on their transitions over states by clustering. 
Below we describe these two steps in detail.

\subsection{Identify Gamers' Playing States Using HMM Model}
We assume that there is an underlying playing state that controls
userâ€™s behaviors at each time period, and the state evolves  as the player changes his strategy over
time. We represent each gamers' data using a Hidden Markov Model (HMM) \cite{hmm} chain with 
length $T$, the total time periods.  Let $Y_{it}$ be the hidden state representing 
the $i$-th gamer's underlying playing state at time $t$, and $Y_{it}$ is discrete which can take on $S$ values $\{1, \cdots, S\}$.

%The gamer's HMM model has three parameters: 1) the initial state distribution vector $\pi$ 
%with $\pi_s$ being the probability of a gamer starting with state $s$; 
%2) the state transition parameter $A$, with $A_{rs}$ being the probability of a gamer transitioning from state
%$r$ at time $t-1$ to state $s$ at time $t$. 3) the emission parameter $B$, with $B_s$ controlling the probability of observing $X_{it}$ at state $Y_{it} = s$. 

The mechanism of the gamers' HMM model is as below: 1) initially, a gamer starts with state 
$Y_{i1} \in \{1, \cdots, S\}$ according to an initial distribution $\pi$,  with $\pi_s$ being 
the probability of starting at state $s$; 2) all the $Y_{it}$'s  evolves according to the 
{\it Markov property}: given $Y_{i{t-1}}$, the state $Y_{it}$ is independent of all the 
states prior to $t-1$, and the transition matrix is $A$, with $A_{rs}$ being the probability of 
transitioning from state $r$ to state $t$; 3) at each time $t$, the observations $X_{it}$ only 
depends on the state $Y_{it}$ parametrized by $B$, with $B_s$ controlling the probability 
of observing $X_{it}$ at state $Y_{it} = s$. 


Given the observed data $X$ and the number of states $S$, we can estimate the parameters by maximizing the 
likelihood of the observations
\[
  \max_{\pi, A, B} P(X | \pi, A, B)
= \prod_{i=1}^N P(Y_{i1} | \pi) P(X_{i1} | Y_{i1}, B)
  \prod_{t=2}^T P(Y_{it} | Y_{i{t-1}}, A) P(X_{it} | Y_{it}, B) ~.
\]
However, in the application of modeling gamers' playing states, the number of states $S$ is usually unknown. 
We will estimate $S$ using criteria such as changes of data likelihood, AIC or BIC. 
 
After estimating the parameters, the state sequence $Y_{i1}, \cdots, Y_{iT}$ 
for each user can be found using Viterbi decoding \cite{hmm} which maximizes
$P(Y_{i1}, \cdots, Y_{iT}| X, \pi, A, B)$. 
%One possible solution is to estimate $Y_{it}$ individually by maximizing $P(Y_{it}| X_{it}, \pi, A, B)$.

\subsection{Clustering Gamers based on State Transitions}
\label{sec:clustering}
The next step is to clustering the gamers based on their trends of state transitions. 
Here we adopt a similar method that has been successfully applied to crisis identification in distributed computing system \cite{moises}. 
Let $Y = [Y_1, \cdots, Y_N]^\top$ be the state transitions for all the players,
where $Y_i = [Y_{i1}, \cdots, Y_{iT}]^\top$ denotes the $i$-th player's states from 1 to
the $T$-th time. We denote $Z_i$ as the underlying cluster assignment for the $i$-th player. 
Let $K$ be the total number of (unknown) clusters, which we will found automatically.

\subsubsection{The Clustering Model}
We adopt the mixture of Dirichlet process model (DP) for clustering \cite{dpclustering}. 
Suppose the clusters evolves according to a Dirichlet distribution with parameter $\alpha$.
We assume that each cluster $k$ generates a Markov chain parametrized by $\{\lambda^k, \Phi^k\}$, 
where $\lambda^k$ is the $S\time 1$ vector for the initial state distribution, 
and $\Phi^k$ is the $S \times S$ transition matrix. We use the prior distribution 
for parameters in each cluster is $G_0(\{\lambda^k, \Phi^k\}) = Dir(\hat\pi) \prod_{s=1}^S Dir (\hat B_{s.})$,
where $\hat\pi$ and $\hat B$ are the estimated parameters at the first step. 
The conditional probability  
\begin{equation}
\label{eq:condi}
  P(\{\lambda^k, \Phi^k \}_{k=1}^K | Z ) 
= \prod_k G_0(\{\lambda^k, \Phi^k\})~.
\end{equation}

%The data generation process is assumed as below \\
%\begin{tabular}{l}
%    \quad For each player $i$,\\
%    \qquad 1. Sample cluster assignment $Z_i \sim Dir(\alpha)$  \\
%    \qquad 2. Generate the state sequence $Y_{i1}, \cdots Y_{iT}$ with Markov chain parametrized by $\{\lambda^{Z_i}, \Phi^{Z_i}\}$.
%\end{tabular}
 

Given the clustering model, the likelihood of the data of state transitions for all players is
\begin{equation}
\label{eq:likeli}
  P(Y| Z, \{\lambda^k, \Phi^k \}_{k=1}^K) 
= \prod_{i=1}^N \left( \prod_{s=1}^S \lambda^{\mathbf{1}[Y_{i1} = s]} \prod_{r=1}^S \left(\Phi_{rs}^{Z_i}\right)^{n_{irs}} \right)~,
\end{equation}
where $\mathbf{1} [\cdot]$ is the indicator function, and $n_{irs}$ is the number of transitions from state $r$ to state $s$ for the $i$-th player.

\subsubsection{MCMC Sampling of Posterior Distribution}
Here we use a collapsed-space sampling method \cite{neal2000markov,collapsed_escobar} to obtain samples from the reduced-spaced posterior distribution $P(Z|Y)$, instead of the full-space distribution $P(Z, \{\lambda, \Phi\} | Y)$. This allows for easy sampling steps and faster convergence rate. 
The reduced-space posterior distribution is
\begin{equation*}
 P(Z|Y) \propto P(Z, Y) = P(Y|Z) P(Z).
\end{equation*}
The likelihood $P(Y|Z)$ can be computed by integrating out the cluster-specific parameters $\{\lambda^k, \Phi^k\}_{k=1}^K$.
Substituting  (\ref{eq:condi}) and (\ref{eq:likeli}), we obtain
%\begin{equation*}
%\renewcommand*{\arraystretch}{1.8}
%\begin{array}{rl}
%     P(Y|Z) 
%~ = & \int  P(Y| Z, \{\lambda^k, \Phi^k \}_{k=1}^K) P(\lambda^k, \Phi^k | Z) d\lambda^k d\Phi^{k} \\
%~ = & \displaystyle{ \prod_{k=1}^K 
%      \left[ 
%      \frac{\prod_s \Gamma(\hat \pi_s + \sum_{i} \mathbf{1}[Z_i=k,Y_{i1} = s]) \Gamma (\sum_s \hat \pi_s)}
%           {\Gamma\left( \sum_s (\hat  \pi_s + \sum_{i} \mathbf{1}[Z_i=k,Y_{i1} = s]) \right) \prod_s \Gamma (\hat \pi_s)}
%      \right]  } \times \\
% &  \displaystyle{ \prod_{k=1}^K \prod_r 
%      \left[ 
%      \frac{\prod_s \Gamma(\hat B_{rs} + \sum_{i} \mathbf{1}[Z_i=k] n_{irs} ) \Gamma (\sum_s \hat B_{rs})}
%           {\Gamma\left( \sum_s (\hat  B_{rs} + \sum_{i}\mathbf{1}[Z_i=k]  n_{irs} ) \right) \prod_s \Gamma (\hat B_{rs})}
%      \right] } ~,
%\end{array}
%\end{equation*}
\begin{equation*}
\renewcommand*{\arraystretch}{1.9}
\begin{array}{rl}
     P(Y|Z) 
~ = & \int  P(Y| Z, \{\lambda^k, \Phi^k \}_{k=1}^K) P(\lambda^k, \Phi^k | Z) d\lambda^k d\Phi^{k} \\
~ = & \displaystyle{ \prod_{k=1}^K 
      \left[ 
      \frac{\prod_s \Gamma(\bar \pi_s) \Gamma (\sum_s \hat \pi_s)}
           {\Gamma\left( \sum_s \bar \pi_s \right) \prod_s \Gamma (\hat \pi_s)}
      \right]  } \times
    \displaystyle{ \prod_{k=1}^K \prod_r 
      \left[ 
      \frac{\prod_s \Gamma(\bar B_{rs}) \Gamma (\sum_s \hat B_{rs})}
           {\Gamma\left( \sum_s \bar  B_{rs} \right) \prod_s \Gamma (\hat B_{rs})}
      \right] } ~,
\end{array}
\end{equation*}
where
$\bar \pi _s = \hat \pi_s + \sum_{i} \mathbf{1}[Z_i=k,Y_{i1} = s]$, and
$\bar B_{rs} = \hat B_{rs} + \sum_{i} n_{irs}  \cdot \mathbf{1}[Z_i=k]$.

Sampling $Z$ from Dirichlet distribution can be equivalently done as below \cite{neal2000markov}: 
set $Z_1 = 1$; for subsequent players, sample $Z_i$ according to  the following distribution 
\begin{equation*}
\label{eq:condition}
\renewcommand*{\arraystretch}{1.5}
\begin{array}{rl}
    P(Z_i = k | Z_1, \cdots, Z_{i-1}) 
~~=& \frac{|\{i' < i : Z_{i'} = k\}|}{i-1 + \alpha}  ~, \quad \text{for } k \in \{Z_{i'}\}_{i' < i}  \\
   P(Z_i = Z_{i'}, \forall i' < i | Z_1, \cdots, Z_{i-1}) 
~~=& \frac{\alpha}{i-1 + \alpha} ~,
\end{array}
\end{equation*}
where $|\cdot|$ denotes the number of elements in a set.

Note that our clustering method can be easily extended to incorporate side information 
such as pairwise constraints \cite{constraintBook}, by only considering the samples $Z$ that
satisfies the constraints.

